{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "proj3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhkRtz1a-8pD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTbYCh8IZFot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY6qEmZIZIjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper parameters\n",
        "num_epochs = 300\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONU05VQqrSft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize = transforms.Normalize(mean=[.5, .5, .5],\n",
        "                                 std=[1 ,1, 1])\n",
        "transform_ = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmdQBCY9_Xpd",
        "colab_type": "code",
        "outputId": "9d800c45-3c34-48ad-ad2d-a47c942080c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Download and construct CIFAR-10 dataset.\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                             train=True, \n",
        "                                             transform=transform_,\n",
        "                                             download=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:06, 28082388.76it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../data/cifar-10-python.tar.gz to ../../data/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y5Y741Zj6Xy",
        "colab_type": "code",
        "outputId": "e1ec2ac0-29ff-4ab8-eeb4-432b01aecec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "image, label = train_dataset[0]\n",
        "print (image.size())\n",
        "print (label)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlgqBFCll2Xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSZ-bByakWpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data loader (this provides queues and threads in a very simple way).\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,   \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True,\n",
        "                                          )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpZTM-ejkioH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data_iter = iter(train_loader)\n",
        "# images, labels = data_iter.next()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icy3ZOD1qdTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# images.max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZTsyrC7s69q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m89udSOAwAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional neural network (two convolutional layers)\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 24, kernel_size=5, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(48, 96, kernel_size=3, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(2*2*96, 1024),\n",
        "            nn.ReLU())\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "        self.sm = nn.Softmax()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sm(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j69GtTxEZjUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ConvNet(num_classes).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJAXw-qWZvxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #if needed momentum=0.9\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIHm5yQWbEvF",
        "colab_type": "code",
        "outputId": "06380fa7-d715-483f-92dd-23af1dd4a904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/300], Step [100/782], Loss: 2.3025\n",
            "Epoch [1/300], Step [200/782], Loss: 2.2984\n",
            "Epoch [1/300], Step [300/782], Loss: 2.3002\n",
            "Epoch [1/300], Step [400/782], Loss: 2.2974\n",
            "Epoch [1/300], Step [500/782], Loss: 2.2957\n",
            "Epoch [1/300], Step [600/782], Loss: 2.2942\n",
            "Epoch [1/300], Step [700/782], Loss: 2.2980\n",
            "Epoch [2/300], Step [100/782], Loss: 2.2915\n",
            "Epoch [2/300], Step [200/782], Loss: 2.2900\n",
            "Epoch [2/300], Step [300/782], Loss: 2.2811\n",
            "Epoch [2/300], Step [400/782], Loss: 2.2805\n",
            "Epoch [2/300], Step [500/782], Loss: 2.2793\n",
            "Epoch [2/300], Step [600/782], Loss: 2.2878\n",
            "Epoch [2/300], Step [700/782], Loss: 2.2754\n",
            "Epoch [3/300], Step [100/782], Loss: 2.2887\n",
            "Epoch [3/300], Step [200/782], Loss: 2.2757\n",
            "Epoch [3/300], Step [300/782], Loss: 2.2592\n",
            "Epoch [3/300], Step [400/782], Loss: 2.2707\n",
            "Epoch [3/300], Step [500/782], Loss: 2.2553\n",
            "Epoch [3/300], Step [600/782], Loss: 2.2450\n",
            "Epoch [3/300], Step [700/782], Loss: 2.2474\n",
            "Epoch [4/300], Step [100/782], Loss: 2.2684\n",
            "Epoch [4/300], Step [200/782], Loss: 2.2564\n",
            "Epoch [4/300], Step [300/782], Loss: 2.2669\n",
            "Epoch [4/300], Step [400/782], Loss: 2.2531\n",
            "Epoch [4/300], Step [500/782], Loss: 2.2443\n",
            "Epoch [4/300], Step [600/782], Loss: 2.2371\n",
            "Epoch [4/300], Step [700/782], Loss: 2.2263\n",
            "Epoch [5/300], Step [100/782], Loss: 2.2223\n",
            "Epoch [5/300], Step [200/782], Loss: 2.2312\n",
            "Epoch [5/300], Step [300/782], Loss: 2.2386\n",
            "Epoch [5/300], Step [400/782], Loss: 2.2229\n",
            "Epoch [5/300], Step [500/782], Loss: 2.2212\n",
            "Epoch [5/300], Step [600/782], Loss: 2.2435\n",
            "Epoch [5/300], Step [700/782], Loss: 2.2186\n",
            "Epoch [6/300], Step [100/782], Loss: 2.1954\n",
            "Epoch [6/300], Step [200/782], Loss: 2.2739\n",
            "Epoch [6/300], Step [300/782], Loss: 2.2296\n",
            "Epoch [6/300], Step [400/782], Loss: 2.2145\n",
            "Epoch [6/300], Step [500/782], Loss: 2.1841\n",
            "Epoch [6/300], Step [600/782], Loss: 2.2342\n",
            "Epoch [6/300], Step [700/782], Loss: 2.1904\n",
            "Epoch [7/300], Step [100/782], Loss: 2.1903\n",
            "Epoch [7/300], Step [200/782], Loss: 2.2248\n",
            "Epoch [7/300], Step [300/782], Loss: 2.1847\n",
            "Epoch [7/300], Step [400/782], Loss: 2.1861\n",
            "Epoch [7/300], Step [500/782], Loss: 2.1581\n",
            "Epoch [7/300], Step [600/782], Loss: 2.2151\n",
            "Epoch [7/300], Step [700/782], Loss: 2.2352\n",
            "Epoch [8/300], Step [100/782], Loss: 2.2187\n",
            "Epoch [8/300], Step [200/782], Loss: 2.1907\n",
            "Epoch [8/300], Step [300/782], Loss: 2.1751\n",
            "Epoch [8/300], Step [400/782], Loss: 2.1532\n",
            "Epoch [8/300], Step [500/782], Loss: 2.1697\n",
            "Epoch [8/300], Step [600/782], Loss: 2.2043\n",
            "Epoch [8/300], Step [700/782], Loss: 2.1877\n",
            "Epoch [9/300], Step [100/782], Loss: 2.1856\n",
            "Epoch [9/300], Step [200/782], Loss: 2.1674\n",
            "Epoch [9/300], Step [300/782], Loss: 2.1579\n",
            "Epoch [9/300], Step [400/782], Loss: 2.1618\n",
            "Epoch [9/300], Step [500/782], Loss: 2.1320\n",
            "Epoch [9/300], Step [600/782], Loss: 2.1878\n",
            "Epoch [9/300], Step [700/782], Loss: 2.1414\n",
            "Epoch [10/300], Step [100/782], Loss: 2.1427\n",
            "Epoch [10/300], Step [200/782], Loss: 2.1348\n",
            "Epoch [10/300], Step [300/782], Loss: 2.1361\n",
            "Epoch [10/300], Step [400/782], Loss: 2.1589\n",
            "Epoch [10/300], Step [500/782], Loss: 2.1729\n",
            "Epoch [10/300], Step [600/782], Loss: 2.1010\n",
            "Epoch [10/300], Step [700/782], Loss: 2.1581\n",
            "Epoch [11/300], Step [100/782], Loss: 2.1261\n",
            "Epoch [11/300], Step [200/782], Loss: 2.1523\n",
            "Epoch [11/300], Step [300/782], Loss: 2.1648\n",
            "Epoch [11/300], Step [400/782], Loss: 2.1584\n",
            "Epoch [11/300], Step [500/782], Loss: 2.1264\n",
            "Epoch [11/300], Step [600/782], Loss: 2.1125\n",
            "Epoch [11/300], Step [700/782], Loss: 2.0652\n",
            "Epoch [12/300], Step [100/782], Loss: 2.1207\n",
            "Epoch [12/300], Step [200/782], Loss: 2.0670\n",
            "Epoch [12/300], Step [300/782], Loss: 2.1176\n",
            "Epoch [12/300], Step [400/782], Loss: 2.2027\n",
            "Epoch [12/300], Step [500/782], Loss: 2.0418\n",
            "Epoch [12/300], Step [600/782], Loss: 2.1455\n",
            "Epoch [12/300], Step [700/782], Loss: 2.1345\n",
            "Epoch [13/300], Step [100/782], Loss: 2.0857\n",
            "Epoch [13/300], Step [200/782], Loss: 2.1118\n",
            "Epoch [13/300], Step [300/782], Loss: 2.1211\n",
            "Epoch [13/300], Step [400/782], Loss: 2.0657\n",
            "Epoch [13/300], Step [500/782], Loss: 2.1292\n",
            "Epoch [13/300], Step [600/782], Loss: 2.0404\n",
            "Epoch [13/300], Step [700/782], Loss: 2.1389\n",
            "Epoch [14/300], Step [100/782], Loss: 1.9859\n",
            "Epoch [14/300], Step [200/782], Loss: 2.1143\n",
            "Epoch [14/300], Step [300/782], Loss: 2.1379\n",
            "Epoch [14/300], Step [400/782], Loss: 2.1993\n",
            "Epoch [14/300], Step [500/782], Loss: 2.1408\n",
            "Epoch [14/300], Step [600/782], Loss: 2.1544\n",
            "Epoch [14/300], Step [700/782], Loss: 2.0623\n",
            "Epoch [15/300], Step [100/782], Loss: 2.1382\n",
            "Epoch [15/300], Step [200/782], Loss: 2.1019\n",
            "Epoch [15/300], Step [300/782], Loss: 2.1255\n",
            "Epoch [15/300], Step [400/782], Loss: 2.1020\n",
            "Epoch [15/300], Step [500/782], Loss: 2.0998\n",
            "Epoch [15/300], Step [600/782], Loss: 2.0204\n",
            "Epoch [15/300], Step [700/782], Loss: 2.0873\n",
            "Epoch [16/300], Step [100/782], Loss: 2.1007\n",
            "Epoch [16/300], Step [200/782], Loss: 2.1261\n",
            "Epoch [16/300], Step [300/782], Loss: 2.1261\n",
            "Epoch [16/300], Step [400/782], Loss: 2.0934\n",
            "Epoch [16/300], Step [500/782], Loss: 2.0713\n",
            "Epoch [16/300], Step [600/782], Loss: 2.0468\n",
            "Epoch [16/300], Step [700/782], Loss: 2.1132\n",
            "Epoch [17/300], Step [100/782], Loss: 2.0547\n",
            "Epoch [17/300], Step [200/782], Loss: 2.0005\n",
            "Epoch [17/300], Step [300/782], Loss: 2.0923\n",
            "Epoch [17/300], Step [400/782], Loss: 2.1072\n",
            "Epoch [17/300], Step [500/782], Loss: 2.0027\n",
            "Epoch [17/300], Step [600/782], Loss: 2.0893\n",
            "Epoch [17/300], Step [700/782], Loss: 2.1457\n",
            "Epoch [18/300], Step [100/782], Loss: 2.0692\n",
            "Epoch [18/300], Step [200/782], Loss: 2.0730\n",
            "Epoch [18/300], Step [300/782], Loss: 2.0263\n",
            "Epoch [18/300], Step [400/782], Loss: 2.0644\n",
            "Epoch [18/300], Step [500/782], Loss: 2.1485\n",
            "Epoch [18/300], Step [600/782], Loss: 2.1018\n",
            "Epoch [18/300], Step [700/782], Loss: 1.9968\n",
            "Epoch [19/300], Step [100/782], Loss: 2.0059\n",
            "Epoch [19/300], Step [200/782], Loss: 2.0922\n",
            "Epoch [19/300], Step [300/782], Loss: 2.0473\n",
            "Epoch [19/300], Step [400/782], Loss: 1.9933\n",
            "Epoch [19/300], Step [500/782], Loss: 2.0400\n",
            "Epoch [19/300], Step [600/782], Loss: 2.0254\n",
            "Epoch [19/300], Step [700/782], Loss: 1.9865\n",
            "Epoch [20/300], Step [100/782], Loss: 2.0449\n",
            "Epoch [20/300], Step [200/782], Loss: 2.0460\n",
            "Epoch [20/300], Step [300/782], Loss: 2.1147\n",
            "Epoch [20/300], Step [400/782], Loss: 2.0662\n",
            "Epoch [20/300], Step [500/782], Loss: 2.0891\n",
            "Epoch [20/300], Step [600/782], Loss: 2.0486\n",
            "Epoch [20/300], Step [700/782], Loss: 2.0581\n",
            "Epoch [21/300], Step [100/782], Loss: 2.0683\n",
            "Epoch [21/300], Step [200/782], Loss: 2.1173\n",
            "Epoch [21/300], Step [300/782], Loss: 2.0354\n",
            "Epoch [21/300], Step [400/782], Loss: 2.1174\n",
            "Epoch [21/300], Step [500/782], Loss: 2.0862\n",
            "Epoch [21/300], Step [600/782], Loss: 2.0887\n",
            "Epoch [21/300], Step [700/782], Loss: 1.9254\n",
            "Epoch [22/300], Step [100/782], Loss: 2.0642\n",
            "Epoch [22/300], Step [200/782], Loss: 2.0875\n",
            "Epoch [22/300], Step [300/782], Loss: 2.0915\n",
            "Epoch [22/300], Step [400/782], Loss: 2.0268\n",
            "Epoch [22/300], Step [500/782], Loss: 2.0027\n",
            "Epoch [22/300], Step [600/782], Loss: 2.0487\n",
            "Epoch [22/300], Step [700/782], Loss: 2.0189\n",
            "Epoch [23/300], Step [100/782], Loss: 2.0032\n",
            "Epoch [23/300], Step [200/782], Loss: 2.0120\n",
            "Epoch [23/300], Step [300/782], Loss: 2.0336\n",
            "Epoch [23/300], Step [400/782], Loss: 2.0637\n",
            "Epoch [23/300], Step [500/782], Loss: 1.9787\n",
            "Epoch [23/300], Step [600/782], Loss: 2.0135\n",
            "Epoch [23/300], Step [700/782], Loss: 2.1047\n",
            "Epoch [24/300], Step [100/782], Loss: 1.9989\n",
            "Epoch [24/300], Step [200/782], Loss: 1.9858\n",
            "Epoch [24/300], Step [300/782], Loss: 2.0721\n",
            "Epoch [24/300], Step [400/782], Loss: 1.9960\n",
            "Epoch [24/300], Step [500/782], Loss: 2.0139\n",
            "Epoch [24/300], Step [600/782], Loss: 2.0329\n",
            "Epoch [24/300], Step [700/782], Loss: 2.1059\n",
            "Epoch [25/300], Step [100/782], Loss: 1.9862\n",
            "Epoch [25/300], Step [200/782], Loss: 2.0355\n",
            "Epoch [25/300], Step [300/782], Loss: 2.0873\n",
            "Epoch [25/300], Step [400/782], Loss: 2.0358\n",
            "Epoch [25/300], Step [500/782], Loss: 2.0199\n",
            "Epoch [25/300], Step [600/782], Loss: 2.0026\n",
            "Epoch [25/300], Step [700/782], Loss: 1.9845\n",
            "Epoch [26/300], Step [100/782], Loss: 1.9913\n",
            "Epoch [26/300], Step [200/782], Loss: 2.0001\n",
            "Epoch [26/300], Step [300/782], Loss: 2.0492\n",
            "Epoch [26/300], Step [400/782], Loss: 2.0944\n",
            "Epoch [26/300], Step [500/782], Loss: 1.9389\n",
            "Epoch [26/300], Step [600/782], Loss: 1.9956\n",
            "Epoch [26/300], Step [700/782], Loss: 1.9855\n",
            "Epoch [27/300], Step [100/782], Loss: 2.0712\n",
            "Epoch [27/300], Step [200/782], Loss: 2.0540\n",
            "Epoch [27/300], Step [300/782], Loss: 2.0564\n",
            "Epoch [27/300], Step [400/782], Loss: 1.9981\n",
            "Epoch [27/300], Step [500/782], Loss: 2.0145\n",
            "Epoch [27/300], Step [600/782], Loss: 2.0631\n",
            "Epoch [27/300], Step [700/782], Loss: 2.0046\n",
            "Epoch [28/300], Step [100/782], Loss: 2.0189\n",
            "Epoch [28/300], Step [200/782], Loss: 1.9961\n",
            "Epoch [28/300], Step [300/782], Loss: 2.0040\n",
            "Epoch [28/300], Step [400/782], Loss: 2.0093\n",
            "Epoch [28/300], Step [500/782], Loss: 1.9675\n",
            "Epoch [28/300], Step [600/782], Loss: 1.9588\n",
            "Epoch [28/300], Step [700/782], Loss: 2.0649\n",
            "Epoch [29/300], Step [100/782], Loss: 1.9813\n",
            "Epoch [29/300], Step [200/782], Loss: 1.9698\n",
            "Epoch [29/300], Step [300/782], Loss: 1.9892\n",
            "Epoch [29/300], Step [400/782], Loss: 2.0496\n",
            "Epoch [29/300], Step [500/782], Loss: 2.0277\n",
            "Epoch [29/300], Step [600/782], Loss: 1.9663\n",
            "Epoch [29/300], Step [700/782], Loss: 1.9505\n",
            "Epoch [30/300], Step [100/782], Loss: 1.9253\n",
            "Epoch [30/300], Step [200/782], Loss: 2.0503\n",
            "Epoch [30/300], Step [300/782], Loss: 1.9304\n",
            "Epoch [30/300], Step [400/782], Loss: 1.9692\n",
            "Epoch [30/300], Step [500/782], Loss: 1.9535\n",
            "Epoch [30/300], Step [600/782], Loss: 2.0614\n",
            "Epoch [30/300], Step [700/782], Loss: 2.0110\n",
            "Epoch [31/300], Step [100/782], Loss: 2.1004\n",
            "Epoch [31/300], Step [200/782], Loss: 1.9501\n",
            "Epoch [31/300], Step [300/782], Loss: 1.9867\n",
            "Epoch [31/300], Step [400/782], Loss: 1.9407\n",
            "Epoch [31/300], Step [500/782], Loss: 1.9658\n",
            "Epoch [31/300], Step [600/782], Loss: 2.0178\n",
            "Epoch [31/300], Step [700/782], Loss: 1.9938\n",
            "Epoch [32/300], Step [100/782], Loss: 1.9894\n",
            "Epoch [32/300], Step [200/782], Loss: 1.9928\n",
            "Epoch [32/300], Step [300/782], Loss: 1.9874\n",
            "Epoch [32/300], Step [400/782], Loss: 1.9512\n",
            "Epoch [32/300], Step [500/782], Loss: 2.0124\n",
            "Epoch [32/300], Step [600/782], Loss: 2.0068\n",
            "Epoch [32/300], Step [700/782], Loss: 2.0112\n",
            "Epoch [33/300], Step [100/782], Loss: 1.9589\n",
            "Epoch [33/300], Step [200/782], Loss: 2.0553\n",
            "Epoch [33/300], Step [300/782], Loss: 2.0030\n",
            "Epoch [33/300], Step [400/782], Loss: 1.9836\n",
            "Epoch [33/300], Step [500/782], Loss: 1.9566\n",
            "Epoch [33/300], Step [600/782], Loss: 1.9725\n",
            "Epoch [33/300], Step [700/782], Loss: 1.9749\n",
            "Epoch [34/300], Step [100/782], Loss: 2.0407\n",
            "Epoch [34/300], Step [200/782], Loss: 1.9929\n",
            "Epoch [34/300], Step [300/782], Loss: 1.9100\n",
            "Epoch [34/300], Step [400/782], Loss: 2.0742\n",
            "Epoch [34/300], Step [500/782], Loss: 1.9546\n",
            "Epoch [34/300], Step [600/782], Loss: 1.9794\n",
            "Epoch [34/300], Step [700/782], Loss: 2.0834\n",
            "Epoch [35/300], Step [100/782], Loss: 2.0012\n",
            "Epoch [35/300], Step [200/782], Loss: 1.9099\n",
            "Epoch [35/300], Step [300/782], Loss: 1.9525\n",
            "Epoch [35/300], Step [400/782], Loss: 1.9650\n",
            "Epoch [35/300], Step [500/782], Loss: 2.0222\n",
            "Epoch [35/300], Step [600/782], Loss: 1.9246\n",
            "Epoch [35/300], Step [700/782], Loss: 1.9737\n",
            "Epoch [36/300], Step [100/782], Loss: 1.9630\n",
            "Epoch [36/300], Step [200/782], Loss: 1.9635\n",
            "Epoch [36/300], Step [300/782], Loss: 2.0465\n",
            "Epoch [36/300], Step [400/782], Loss: 1.9876\n",
            "Epoch [36/300], Step [500/782], Loss: 1.9900\n",
            "Epoch [36/300], Step [600/782], Loss: 1.9970\n",
            "Epoch [36/300], Step [700/782], Loss: 1.9341\n",
            "Epoch [37/300], Step [100/782], Loss: 1.9401\n",
            "Epoch [37/300], Step [200/782], Loss: 1.9207\n",
            "Epoch [37/300], Step [300/782], Loss: 1.9598\n",
            "Epoch [37/300], Step [400/782], Loss: 1.9987\n",
            "Epoch [37/300], Step [500/782], Loss: 2.0510\n",
            "Epoch [37/300], Step [600/782], Loss: 2.0152\n",
            "Epoch [37/300], Step [700/782], Loss: 1.9165\n",
            "Epoch [38/300], Step [100/782], Loss: 1.9747\n",
            "Epoch [38/300], Step [200/782], Loss: 1.8986\n",
            "Epoch [38/300], Step [300/782], Loss: 2.0175\n",
            "Epoch [38/300], Step [400/782], Loss: 1.9641\n",
            "Epoch [38/300], Step [500/782], Loss: 1.9612\n",
            "Epoch [38/300], Step [600/782], Loss: 1.9474\n",
            "Epoch [38/300], Step [700/782], Loss: 2.0453\n",
            "Epoch [39/300], Step [100/782], Loss: 1.9421\n",
            "Epoch [39/300], Step [200/782], Loss: 1.8894\n",
            "Epoch [39/300], Step [300/782], Loss: 1.9532\n",
            "Epoch [39/300], Step [400/782], Loss: 2.0160\n",
            "Epoch [39/300], Step [500/782], Loss: 1.9751\n",
            "Epoch [39/300], Step [600/782], Loss: 1.9600\n",
            "Epoch [39/300], Step [700/782], Loss: 1.9284\n",
            "Epoch [40/300], Step [100/782], Loss: 1.9671\n",
            "Epoch [40/300], Step [200/782], Loss: 2.0106\n",
            "Epoch [40/300], Step [300/782], Loss: 1.9561\n",
            "Epoch [40/300], Step [400/782], Loss: 1.8669\n",
            "Epoch [40/300], Step [500/782], Loss: 1.9500\n",
            "Epoch [40/300], Step [600/782], Loss: 1.8812\n",
            "Epoch [40/300], Step [700/782], Loss: 2.0140\n",
            "Epoch [41/300], Step [100/782], Loss: 1.9594\n",
            "Epoch [41/300], Step [200/782], Loss: 1.9251\n",
            "Epoch [41/300], Step [300/782], Loss: 1.9999\n",
            "Epoch [41/300], Step [400/782], Loss: 1.9992\n",
            "Epoch [41/300], Step [500/782], Loss: 1.8378\n",
            "Epoch [41/300], Step [600/782], Loss: 1.9715\n",
            "Epoch [41/300], Step [700/782], Loss: 1.9746\n",
            "Epoch [42/300], Step [100/782], Loss: 1.8978\n",
            "Epoch [42/300], Step [200/782], Loss: 1.9498\n",
            "Epoch [42/300], Step [300/782], Loss: 1.8238\n",
            "Epoch [42/300], Step [400/782], Loss: 1.8935\n",
            "Epoch [42/300], Step [500/782], Loss: 1.9142\n",
            "Epoch [42/300], Step [600/782], Loss: 1.9764\n",
            "Epoch [42/300], Step [700/782], Loss: 1.8754\n",
            "Epoch [43/300], Step [100/782], Loss: 1.9261\n",
            "Epoch [43/300], Step [200/782], Loss: 1.9314\n",
            "Epoch [43/300], Step [300/782], Loss: 1.9101\n",
            "Epoch [43/300], Step [400/782], Loss: 1.8534\n",
            "Epoch [43/300], Step [500/782], Loss: 1.9679\n",
            "Epoch [43/300], Step [600/782], Loss: 1.9870\n",
            "Epoch [43/300], Step [700/782], Loss: 1.9253\n",
            "Epoch [44/300], Step [100/782], Loss: 1.9085\n",
            "Epoch [44/300], Step [200/782], Loss: 1.9553\n",
            "Epoch [44/300], Step [300/782], Loss: 1.9647\n",
            "Epoch [44/300], Step [400/782], Loss: 2.0266\n",
            "Epoch [44/300], Step [500/782], Loss: 2.0128\n",
            "Epoch [44/300], Step [600/782], Loss: 1.8847\n",
            "Epoch [44/300], Step [700/782], Loss: 1.9821\n",
            "Epoch [45/300], Step [100/782], Loss: 1.9471\n",
            "Epoch [45/300], Step [200/782], Loss: 1.9707\n",
            "Epoch [45/300], Step [300/782], Loss: 1.9090\n",
            "Epoch [45/300], Step [400/782], Loss: 2.0696\n",
            "Epoch [45/300], Step [500/782], Loss: 1.9385\n",
            "Epoch [45/300], Step [600/782], Loss: 1.8854\n",
            "Epoch [45/300], Step [700/782], Loss: 1.9191\n",
            "Epoch [46/300], Step [100/782], Loss: 1.9177\n",
            "Epoch [46/300], Step [200/782], Loss: 1.9532\n",
            "Epoch [46/300], Step [300/782], Loss: 1.9770\n",
            "Epoch [46/300], Step [400/782], Loss: 1.9668\n",
            "Epoch [46/300], Step [500/782], Loss: 1.8772\n",
            "Epoch [46/300], Step [600/782], Loss: 1.9841\n",
            "Epoch [46/300], Step [700/782], Loss: 1.8531\n",
            "Epoch [47/300], Step [100/782], Loss: 1.9040\n",
            "Epoch [47/300], Step [200/782], Loss: 1.9545\n",
            "Epoch [47/300], Step [300/782], Loss: 1.9539\n",
            "Epoch [47/300], Step [400/782], Loss: 1.9838\n",
            "Epoch [47/300], Step [500/782], Loss: 2.0136\n",
            "Epoch [47/300], Step [600/782], Loss: 1.9073\n",
            "Epoch [47/300], Step [700/782], Loss: 1.9675\n",
            "Epoch [48/300], Step [100/782], Loss: 1.9484\n",
            "Epoch [48/300], Step [200/782], Loss: 1.8948\n",
            "Epoch [48/300], Step [300/782], Loss: 1.9056\n",
            "Epoch [48/300], Step [400/782], Loss: 1.9293\n",
            "Epoch [48/300], Step [500/782], Loss: 1.8720\n",
            "Epoch [48/300], Step [600/782], Loss: 1.8547\n",
            "Epoch [48/300], Step [700/782], Loss: 2.0252\n",
            "Epoch [49/300], Step [100/782], Loss: 2.0038\n",
            "Epoch [49/300], Step [200/782], Loss: 1.8946\n",
            "Epoch [49/300], Step [300/782], Loss: 1.9253\n",
            "Epoch [49/300], Step [400/782], Loss: 2.0320\n",
            "Epoch [49/300], Step [500/782], Loss: 1.9350\n",
            "Epoch [49/300], Step [600/782], Loss: 1.9479\n",
            "Epoch [49/300], Step [700/782], Loss: 1.9711\n",
            "Epoch [50/300], Step [100/782], Loss: 1.8521\n",
            "Epoch [50/300], Step [200/782], Loss: 1.8332\n",
            "Epoch [50/300], Step [300/782], Loss: 1.9126\n",
            "Epoch [50/300], Step [400/782], Loss: 1.9544\n",
            "Epoch [50/300], Step [500/782], Loss: 1.9175\n",
            "Epoch [50/300], Step [600/782], Loss: 1.8906\n",
            "Epoch [50/300], Step [700/782], Loss: 1.9308\n",
            "Epoch [51/300], Step [100/782], Loss: 1.9244\n",
            "Epoch [51/300], Step [200/782], Loss: 1.9062\n",
            "Epoch [51/300], Step [300/782], Loss: 1.8506\n",
            "Epoch [51/300], Step [400/782], Loss: 1.8435\n",
            "Epoch [51/300], Step [500/782], Loss: 1.9104\n",
            "Epoch [51/300], Step [600/782], Loss: 1.9188\n",
            "Epoch [51/300], Step [700/782], Loss: 1.9883\n",
            "Epoch [52/300], Step [100/782], Loss: 1.9118\n",
            "Epoch [52/300], Step [200/782], Loss: 1.8353\n",
            "Epoch [52/300], Step [300/782], Loss: 1.9049\n",
            "Epoch [52/300], Step [400/782], Loss: 1.9377\n",
            "Epoch [52/300], Step [500/782], Loss: 1.8427\n",
            "Epoch [52/300], Step [600/782], Loss: 1.9456\n",
            "Epoch [52/300], Step [700/782], Loss: 1.9301\n",
            "Epoch [53/300], Step [100/782], Loss: 1.8325\n",
            "Epoch [53/300], Step [200/782], Loss: 2.0084\n",
            "Epoch [53/300], Step [300/782], Loss: 1.8477\n",
            "Epoch [53/300], Step [400/782], Loss: 1.8619\n",
            "Epoch [53/300], Step [500/782], Loss: 1.9033\n",
            "Epoch [53/300], Step [600/782], Loss: 1.9868\n",
            "Epoch [53/300], Step [700/782], Loss: 1.9054\n",
            "Epoch [54/300], Step [100/782], Loss: 1.9924\n",
            "Epoch [54/300], Step [200/782], Loss: 1.8226\n",
            "Epoch [54/300], Step [300/782], Loss: 1.9161\n",
            "Epoch [54/300], Step [400/782], Loss: 1.8641\n",
            "Epoch [54/300], Step [500/782], Loss: 1.8603\n",
            "Epoch [54/300], Step [600/782], Loss: 1.9380\n",
            "Epoch [54/300], Step [700/782], Loss: 1.8850\n",
            "Epoch [55/300], Step [100/782], Loss: 1.9278\n",
            "Epoch [55/300], Step [200/782], Loss: 1.8676\n",
            "Epoch [55/300], Step [300/782], Loss: 1.9168\n",
            "Epoch [55/300], Step [400/782], Loss: 1.8671\n",
            "Epoch [55/300], Step [500/782], Loss: 1.9446\n",
            "Epoch [55/300], Step [600/782], Loss: 1.9048\n",
            "Epoch [55/300], Step [700/782], Loss: 1.9697\n",
            "Epoch [56/300], Step [100/782], Loss: 1.8839\n",
            "Epoch [56/300], Step [200/782], Loss: 1.8743\n",
            "Epoch [56/300], Step [300/782], Loss: 1.9076\n",
            "Epoch [56/300], Step [400/782], Loss: 1.9779\n",
            "Epoch [56/300], Step [500/782], Loss: 1.9207\n",
            "Epoch [56/300], Step [600/782], Loss: 1.9106\n",
            "Epoch [56/300], Step [700/782], Loss: 1.9601\n",
            "Epoch [57/300], Step [100/782], Loss: 1.9758\n",
            "Epoch [57/300], Step [200/782], Loss: 1.8871\n",
            "Epoch [57/300], Step [300/782], Loss: 1.8796\n",
            "Epoch [57/300], Step [400/782], Loss: 1.8902\n",
            "Epoch [57/300], Step [500/782], Loss: 1.8410\n",
            "Epoch [57/300], Step [600/782], Loss: 1.9318\n",
            "Epoch [57/300], Step [700/782], Loss: 1.9315\n",
            "Epoch [58/300], Step [100/782], Loss: 1.8810\n",
            "Epoch [58/300], Step [200/782], Loss: 1.9380\n",
            "Epoch [58/300], Step [300/782], Loss: 1.8186\n",
            "Epoch [58/300], Step [400/782], Loss: 1.9177\n",
            "Epoch [58/300], Step [500/782], Loss: 1.9019\n",
            "Epoch [58/300], Step [600/782], Loss: 1.9730\n",
            "Epoch [58/300], Step [700/782], Loss: 1.8930\n",
            "Epoch [59/300], Step [100/782], Loss: 1.8462\n",
            "Epoch [59/300], Step [200/782], Loss: 1.8168\n",
            "Epoch [59/300], Step [300/782], Loss: 1.9304\n",
            "Epoch [59/300], Step [400/782], Loss: 1.8592\n",
            "Epoch [59/300], Step [500/782], Loss: 1.9370\n",
            "Epoch [59/300], Step [600/782], Loss: 1.8900\n",
            "Epoch [59/300], Step [700/782], Loss: 1.8993\n",
            "Epoch [60/300], Step [100/782], Loss: 1.8990\n",
            "Epoch [60/300], Step [200/782], Loss: 1.9465\n",
            "Epoch [60/300], Step [300/782], Loss: 1.8952\n",
            "Epoch [60/300], Step [400/782], Loss: 1.8158\n",
            "Epoch [60/300], Step [500/782], Loss: 1.8552\n",
            "Epoch [60/300], Step [600/782], Loss: 1.8401\n",
            "Epoch [60/300], Step [700/782], Loss: 1.9419\n",
            "Epoch [61/300], Step [100/782], Loss: 1.9113\n",
            "Epoch [61/300], Step [200/782], Loss: 1.9730\n",
            "Epoch [61/300], Step [300/782], Loss: 1.8853\n",
            "Epoch [61/300], Step [400/782], Loss: 1.9444\n",
            "Epoch [61/300], Step [500/782], Loss: 1.9665\n",
            "Epoch [61/300], Step [600/782], Loss: 1.8071\n",
            "Epoch [61/300], Step [700/782], Loss: 1.8844\n",
            "Epoch [62/300], Step [100/782], Loss: 1.9509\n",
            "Epoch [62/300], Step [200/782], Loss: 1.9321\n",
            "Epoch [62/300], Step [300/782], Loss: 1.9223\n",
            "Epoch [62/300], Step [400/782], Loss: 1.9566\n",
            "Epoch [62/300], Step [500/782], Loss: 1.8579\n",
            "Epoch [62/300], Step [600/782], Loss: 1.9657\n",
            "Epoch [62/300], Step [700/782], Loss: 1.9687\n",
            "Epoch [63/300], Step [100/782], Loss: 1.9084\n",
            "Epoch [63/300], Step [200/782], Loss: 1.9057\n",
            "Epoch [63/300], Step [300/782], Loss: 1.8460\n",
            "Epoch [63/300], Step [400/782], Loss: 1.9362\n",
            "Epoch [63/300], Step [500/782], Loss: 1.8975\n",
            "Epoch [63/300], Step [600/782], Loss: 1.8578\n",
            "Epoch [63/300], Step [700/782], Loss: 1.9638\n",
            "Epoch [64/300], Step [100/782], Loss: 1.8306\n",
            "Epoch [64/300], Step [200/782], Loss: 1.8400\n",
            "Epoch [64/300], Step [300/782], Loss: 1.8542\n",
            "Epoch [64/300], Step [400/782], Loss: 1.8252\n",
            "Epoch [64/300], Step [500/782], Loss: 1.8846\n",
            "Epoch [64/300], Step [600/782], Loss: 1.9377\n",
            "Epoch [64/300], Step [700/782], Loss: 1.8053\n",
            "Epoch [65/300], Step [100/782], Loss: 1.8945\n",
            "Epoch [65/300], Step [200/782], Loss: 1.8805\n",
            "Epoch [65/300], Step [300/782], Loss: 1.9176\n",
            "Epoch [65/300], Step [400/782], Loss: 1.8519\n",
            "Epoch [65/300], Step [500/782], Loss: 1.9485\n",
            "Epoch [65/300], Step [600/782], Loss: 1.8533\n",
            "Epoch [65/300], Step [700/782], Loss: 1.9168\n",
            "Epoch [66/300], Step [100/782], Loss: 1.8384\n",
            "Epoch [66/300], Step [200/782], Loss: 1.8746\n",
            "Epoch [66/300], Step [300/782], Loss: 1.8317\n",
            "Epoch [66/300], Step [400/782], Loss: 1.7862\n",
            "Epoch [66/300], Step [500/782], Loss: 1.9355\n",
            "Epoch [66/300], Step [600/782], Loss: 1.9504\n",
            "Epoch [66/300], Step [700/782], Loss: 1.9660\n",
            "Epoch [67/300], Step [100/782], Loss: 1.9137\n",
            "Epoch [67/300], Step [200/782], Loss: 1.8346\n",
            "Epoch [67/300], Step [300/782], Loss: 1.8907\n",
            "Epoch [67/300], Step [400/782], Loss: 1.8801\n",
            "Epoch [67/300], Step [500/782], Loss: 1.9491\n",
            "Epoch [67/300], Step [600/782], Loss: 1.8577\n",
            "Epoch [67/300], Step [700/782], Loss: 1.8348\n",
            "Epoch [68/300], Step [100/782], Loss: 1.8728\n",
            "Epoch [68/300], Step [200/782], Loss: 1.8600\n",
            "Epoch [68/300], Step [300/782], Loss: 1.8975\n",
            "Epoch [68/300], Step [400/782], Loss: 1.9109\n",
            "Epoch [68/300], Step [500/782], Loss: 1.8887\n",
            "Epoch [68/300], Step [600/782], Loss: 1.9941\n",
            "Epoch [68/300], Step [700/782], Loss: 1.9479\n",
            "Epoch [69/300], Step [100/782], Loss: 1.8384\n",
            "Epoch [69/300], Step [200/782], Loss: 1.9667\n",
            "Epoch [69/300], Step [300/782], Loss: 1.8062\n",
            "Epoch [69/300], Step [400/782], Loss: 1.9585\n",
            "Epoch [69/300], Step [500/782], Loss: 1.8571\n",
            "Epoch [69/300], Step [600/782], Loss: 1.9358\n",
            "Epoch [69/300], Step [700/782], Loss: 1.9290\n",
            "Epoch [70/300], Step [100/782], Loss: 1.8449\n",
            "Epoch [70/300], Step [200/782], Loss: 1.9300\n",
            "Epoch [70/300], Step [300/782], Loss: 1.8633\n",
            "Epoch [70/300], Step [400/782], Loss: 1.8800\n",
            "Epoch [70/300], Step [500/782], Loss: 1.8286\n",
            "Epoch [70/300], Step [600/782], Loss: 1.8435\n",
            "Epoch [70/300], Step [700/782], Loss: 1.8411\n",
            "Epoch [71/300], Step [100/782], Loss: 1.9126\n",
            "Epoch [71/300], Step [200/782], Loss: 1.8843\n",
            "Epoch [71/300], Step [300/782], Loss: 1.8247\n",
            "Epoch [71/300], Step [400/782], Loss: 1.8012\n",
            "Epoch [71/300], Step [500/782], Loss: 1.8686\n",
            "Epoch [71/300], Step [600/782], Loss: 1.8697\n",
            "Epoch [71/300], Step [700/782], Loss: 1.8652\n",
            "Epoch [72/300], Step [100/782], Loss: 1.8806\n",
            "Epoch [72/300], Step [200/782], Loss: 1.8593\n",
            "Epoch [72/300], Step [300/782], Loss: 1.8274\n",
            "Epoch [72/300], Step [400/782], Loss: 1.8685\n",
            "Epoch [72/300], Step [500/782], Loss: 1.9906\n",
            "Epoch [72/300], Step [600/782], Loss: 1.8724\n",
            "Epoch [72/300], Step [700/782], Loss: 1.7929\n",
            "Epoch [73/300], Step [100/782], Loss: 1.9716\n",
            "Epoch [73/300], Step [200/782], Loss: 1.8777\n",
            "Epoch [73/300], Step [300/782], Loss: 1.8408\n",
            "Epoch [73/300], Step [400/782], Loss: 1.8372\n",
            "Epoch [73/300], Step [500/782], Loss: 1.8258\n",
            "Epoch [73/300], Step [600/782], Loss: 1.8302\n",
            "Epoch [73/300], Step [700/782], Loss: 1.8219\n",
            "Epoch [74/300], Step [100/782], Loss: 1.8958\n",
            "Epoch [74/300], Step [200/782], Loss: 1.8078\n",
            "Epoch [74/300], Step [300/782], Loss: 1.8000\n",
            "Epoch [74/300], Step [400/782], Loss: 1.8964\n",
            "Epoch [74/300], Step [500/782], Loss: 1.7598\n",
            "Epoch [74/300], Step [600/782], Loss: 1.9045\n",
            "Epoch [74/300], Step [700/782], Loss: 1.7901\n",
            "Epoch [75/300], Step [100/782], Loss: 1.9231\n",
            "Epoch [75/300], Step [200/782], Loss: 1.9163\n",
            "Epoch [75/300], Step [300/782], Loss: 1.9906\n",
            "Epoch [75/300], Step [400/782], Loss: 1.8420\n",
            "Epoch [75/300], Step [500/782], Loss: 1.8509\n",
            "Epoch [75/300], Step [600/782], Loss: 1.7864\n",
            "Epoch [75/300], Step [700/782], Loss: 1.8262\n",
            "Epoch [76/300], Step [100/782], Loss: 1.9121\n",
            "Epoch [76/300], Step [200/782], Loss: 1.9410\n",
            "Epoch [76/300], Step [300/782], Loss: 1.9358\n",
            "Epoch [76/300], Step [400/782], Loss: 1.9053\n",
            "Epoch [76/300], Step [500/782], Loss: 1.9375\n",
            "Epoch [76/300], Step [600/782], Loss: 1.8455\n",
            "Epoch [76/300], Step [700/782], Loss: 1.8759\n",
            "Epoch [77/300], Step [100/782], Loss: 1.8805\n",
            "Epoch [77/300], Step [200/782], Loss: 1.8442\n",
            "Epoch [77/300], Step [300/782], Loss: 1.7996\n",
            "Epoch [77/300], Step [400/782], Loss: 1.8610\n",
            "Epoch [77/300], Step [500/782], Loss: 1.8987\n",
            "Epoch [77/300], Step [600/782], Loss: 1.9517\n",
            "Epoch [77/300], Step [700/782], Loss: 1.8246\n",
            "Epoch [78/300], Step [100/782], Loss: 1.8468\n",
            "Epoch [78/300], Step [200/782], Loss: 1.7448\n",
            "Epoch [78/300], Step [300/782], Loss: 1.8505\n",
            "Epoch [78/300], Step [400/782], Loss: 1.8571\n",
            "Epoch [78/300], Step [500/782], Loss: 1.8652\n",
            "Epoch [78/300], Step [600/782], Loss: 1.8668\n",
            "Epoch [78/300], Step [700/782], Loss: 1.8654\n",
            "Epoch [79/300], Step [100/782], Loss: 1.9696\n",
            "Epoch [79/300], Step [200/782], Loss: 1.8715\n",
            "Epoch [79/300], Step [300/782], Loss: 1.8327\n",
            "Epoch [79/300], Step [400/782], Loss: 1.7995\n",
            "Epoch [79/300], Step [500/782], Loss: 1.8622\n",
            "Epoch [79/300], Step [600/782], Loss: 1.7617\n",
            "Epoch [79/300], Step [700/782], Loss: 1.9084\n",
            "Epoch [80/300], Step [100/782], Loss: 1.8897\n",
            "Epoch [80/300], Step [200/782], Loss: 1.8750\n",
            "Epoch [80/300], Step [300/782], Loss: 1.7873\n",
            "Epoch [80/300], Step [400/782], Loss: 1.7757\n",
            "Epoch [80/300], Step [500/782], Loss: 1.8146\n",
            "Epoch [80/300], Step [600/782], Loss: 1.9339\n",
            "Epoch [80/300], Step [700/782], Loss: 1.9545\n",
            "Epoch [81/300], Step [100/782], Loss: 1.7921\n",
            "Epoch [81/300], Step [200/782], Loss: 1.8356\n",
            "Epoch [81/300], Step [300/782], Loss: 1.8552\n",
            "Epoch [81/300], Step [400/782], Loss: 1.8504\n",
            "Epoch [81/300], Step [500/782], Loss: 1.8814\n",
            "Epoch [81/300], Step [600/782], Loss: 1.9296\n",
            "Epoch [81/300], Step [700/782], Loss: 1.8823\n",
            "Epoch [82/300], Step [100/782], Loss: 1.9572\n",
            "Epoch [82/300], Step [200/782], Loss: 1.8536\n",
            "Epoch [82/300], Step [300/782], Loss: 1.7886\n",
            "Epoch [82/300], Step [400/782], Loss: 1.8467\n",
            "Epoch [82/300], Step [500/782], Loss: 1.8518\n",
            "Epoch [82/300], Step [600/782], Loss: 1.8633\n",
            "Epoch [82/300], Step [700/782], Loss: 2.0002\n",
            "Epoch [83/300], Step [100/782], Loss: 1.8826\n",
            "Epoch [83/300], Step [200/782], Loss: 1.7314\n",
            "Epoch [83/300], Step [300/782], Loss: 1.8048\n",
            "Epoch [83/300], Step [400/782], Loss: 1.8781\n",
            "Epoch [83/300], Step [500/782], Loss: 1.8859\n",
            "Epoch [83/300], Step [600/782], Loss: 1.8742\n",
            "Epoch [83/300], Step [700/782], Loss: 1.9454\n",
            "Epoch [84/300], Step [100/782], Loss: 1.8022\n",
            "Epoch [84/300], Step [200/782], Loss: 1.9443\n",
            "Epoch [84/300], Step [300/782], Loss: 1.9210\n",
            "Epoch [84/300], Step [400/782], Loss: 1.8539\n",
            "Epoch [84/300], Step [500/782], Loss: 1.8677\n",
            "Epoch [84/300], Step [600/782], Loss: 1.8838\n",
            "Epoch [84/300], Step [700/782], Loss: 1.8765\n",
            "Epoch [85/300], Step [100/782], Loss: 1.8415\n",
            "Epoch [85/300], Step [200/782], Loss: 1.8701\n",
            "Epoch [85/300], Step [300/782], Loss: 1.8866\n",
            "Epoch [85/300], Step [400/782], Loss: 1.8833\n",
            "Epoch [85/300], Step [500/782], Loss: 1.8742\n",
            "Epoch [85/300], Step [600/782], Loss: 1.8614\n",
            "Epoch [85/300], Step [700/782], Loss: 1.8592\n",
            "Epoch [86/300], Step [100/782], Loss: 1.8681\n",
            "Epoch [86/300], Step [200/782], Loss: 1.8289\n",
            "Epoch [86/300], Step [300/782], Loss: 1.8389\n",
            "Epoch [86/300], Step [400/782], Loss: 1.8869\n",
            "Epoch [86/300], Step [500/782], Loss: 1.8198\n",
            "Epoch [86/300], Step [600/782], Loss: 1.9106\n",
            "Epoch [86/300], Step [700/782], Loss: 1.9408\n",
            "Epoch [87/300], Step [100/782], Loss: 1.8661\n",
            "Epoch [87/300], Step [200/782], Loss: 1.8282\n",
            "Epoch [87/300], Step [300/782], Loss: 1.7503\n",
            "Epoch [87/300], Step [400/782], Loss: 1.9176\n",
            "Epoch [87/300], Step [500/782], Loss: 1.8920\n",
            "Epoch [87/300], Step [600/782], Loss: 1.7223\n",
            "Epoch [87/300], Step [700/782], Loss: 1.9225\n",
            "Epoch [88/300], Step [100/782], Loss: 1.8385\n",
            "Epoch [88/300], Step [200/782], Loss: 1.8816\n",
            "Epoch [88/300], Step [300/782], Loss: 1.9215\n",
            "Epoch [88/300], Step [400/782], Loss: 1.8684\n",
            "Epoch [88/300], Step [500/782], Loss: 1.8355\n",
            "Epoch [88/300], Step [600/782], Loss: 1.7520\n",
            "Epoch [88/300], Step [700/782], Loss: 1.9295\n",
            "Epoch [89/300], Step [100/782], Loss: 1.8831\n",
            "Epoch [89/300], Step [200/782], Loss: 1.9294\n",
            "Epoch [89/300], Step [300/782], Loss: 1.8292\n",
            "Epoch [89/300], Step [400/782], Loss: 1.8082\n",
            "Epoch [89/300], Step [500/782], Loss: 1.8419\n",
            "Epoch [89/300], Step [600/782], Loss: 1.8599\n",
            "Epoch [89/300], Step [700/782], Loss: 1.8931\n",
            "Epoch [90/300], Step [100/782], Loss: 1.8626\n",
            "Epoch [90/300], Step [200/782], Loss: 1.8142\n",
            "Epoch [90/300], Step [300/782], Loss: 1.9034\n",
            "Epoch [90/300], Step [400/782], Loss: 1.8469\n",
            "Epoch [90/300], Step [500/782], Loss: 1.8797\n",
            "Epoch [90/300], Step [600/782], Loss: 1.8241\n",
            "Epoch [90/300], Step [700/782], Loss: 1.8594\n",
            "Epoch [91/300], Step [100/782], Loss: 1.8270\n",
            "Epoch [91/300], Step [200/782], Loss: 1.8757\n",
            "Epoch [91/300], Step [300/782], Loss: 1.9000\n",
            "Epoch [91/300], Step [400/782], Loss: 1.7867\n",
            "Epoch [91/300], Step [500/782], Loss: 1.8658\n",
            "Epoch [91/300], Step [600/782], Loss: 1.8016\n",
            "Epoch [91/300], Step [700/782], Loss: 1.8419\n",
            "Epoch [92/300], Step [100/782], Loss: 1.7859\n",
            "Epoch [92/300], Step [200/782], Loss: 1.8427\n",
            "Epoch [92/300], Step [300/782], Loss: 1.8709\n",
            "Epoch [92/300], Step [400/782], Loss: 2.0123\n",
            "Epoch [92/300], Step [500/782], Loss: 1.8140\n",
            "Epoch [92/300], Step [600/782], Loss: 1.9100\n",
            "Epoch [92/300], Step [700/782], Loss: 1.7868\n",
            "Epoch [93/300], Step [100/782], Loss: 1.8359\n",
            "Epoch [93/300], Step [200/782], Loss: 1.8524\n",
            "Epoch [93/300], Step [300/782], Loss: 1.8224\n",
            "Epoch [93/300], Step [400/782], Loss: 1.8351\n",
            "Epoch [93/300], Step [500/782], Loss: 1.7648\n",
            "Epoch [93/300], Step [600/782], Loss: 1.8293\n",
            "Epoch [93/300], Step [700/782], Loss: 1.9407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIWpkGaxbGbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the model\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}